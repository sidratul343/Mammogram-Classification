{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sidratul343/Mammogram-Classification/blob/main/Mammogram_Classification%20in%20google%20colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "blessed-configuration",
      "metadata": {
        "id": "blessed-configuration"
      },
      "source": [
        "# Breast Cancer Classification using deep learning model developed by ablation study on pre-processed mammography images\n",
        "\n",
        "In this study, we will be showcasing a computer vision classification problem using mammograms. The dataset we have used can be downloaded from https://wiki.cancerimagingarchive.net/display/Public/CBIS-DDSM. It consists of four classes:\n",
        "\n",
        "* Benign Calcification\n",
        "* Benign Mass\n",
        "* Malignant Calcification\n",
        "* Malignant Mass\n",
        "\n",
        "Therefore, this is a multi-class classification problem. The images containes artifacts and noises which may cause reducing model's performance. Hence, artifacts and noises are removed and images are enhanced by adjusting overall brightness and contrast. After pre-processing the images, the dataset is augmented as the number of images were not sufficient for a deep learning model. Finally, the model is developed using ablation study by altering several components and hyper parameters of CNN architecture.\n",
        "\n",
        "**The figure below showcases examples of four classes of this dataset**\n",
        "\n",
        "![biology-10-01347-g002-550.jpg](attachment:biology-10-01347-g002-550.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "moved-terminal",
      "metadata": {
        "id": "moved-terminal"
      },
      "source": [
        "## Installing software and files\n",
        "\n",
        "To work with computer Vision, we need to install some softwares and files.In this regard, we will be doing all the implementations in Python language on jupyter notebook. To install jupyter notebook and launch other application and files at first we have to download Anaconda which is free.\n",
        "\n",
        "Link to Download Anaconda : https://www.anaconda.com/?modal=nucleus-commercial\n",
        "\n",
        "Guideline for installing Anaconda : https://www.geeksforgeeks.org/how-to-install-anaconda-on-windows/\n",
        "\n",
        "Once Anaconda is downloaded and installed successfully, we may proceed to download Jupyter notebook.\n",
        "\n",
        "## Download and Install Jupyter Notebook\n",
        "\n",
        "\n",
        "Link to download Jupyter using Anaconda : https://docs.anaconda.com/ae-notebooks/4.3.1/user-guide/basic-tasks/apps/jupyter/\n",
        "\n",
        "More informations : https://mas-dse.github.io/startup/anaconda-windows-install/\n",
        "\n",
        "Guideline to use Jupyter notebook : https://www.dataquest.io/blog/jupyter-notebook-tutorial/\n",
        "\n",
        "\n",
        "## Installing Python libraries and packages\n",
        "\n",
        "Most Used Python libraries for image-processing\n",
        "\n",
        "* OpenCV\n",
        "* Scikit-Image\n",
        "* Pillow/PIL\n",
        "* NumPy\n",
        "* Pandas etc\n",
        "* Matplotlib\n",
        "\n",
        "\n",
        "To import and use the packages, we need pip or conda. For now we will be using pip. Using pip on Anaconda command prompt these packages can be installed.\n",
        "\n",
        "**More information**\n",
        "\n",
        "Downloading pip : https://phoenixnap.com/kb/install-pip-windows\n",
        "\n",
        "Install Python packages from Anaconda Prompt using pip : https://datatofish.com/how-to-install-python-package-in-anaconda/\n",
        "\n",
        "Once the packages are installed, you can use them by importing on Jupyter Notebook.\n",
        "\n",
        "\n",
        "Python libraries for Data Science (how to import on notebook) : https://neptune.ai/blog/image-processing-python-libraries-for-machine-learning\n",
        "\n",
        "**** Every Time you restart jupyter notebook, you have to import the libraries again****\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "orange-express",
      "metadata": {
        "id": "orange-express"
      },
      "source": [
        "## Read, display and write image\n",
        "\n",
        "To read an write an image from folder python's Opencv or Pillow can be used, Note that if you read an image using Opencv you have to write the image using opencv as well and same goes for Pillow.\n",
        "\n",
        "To apply Opencv or Pillow we have to import them first.\n",
        "\n",
        "## Using Opencv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "smoking-barrier",
      "metadata": {
        "id": "smoking-barrier",
        "outputId": "d4518725-f0bf-46c7-cdfe-cd17462a6e10"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import cv2\n",
        "\n",
        "read_img_dir = r'D:\\breast\\Benign_Calc\\BenignCalc\\PNG0000.png'  #input image directory\n",
        "\n",
        "read_img = cv2.imread(read_img_dir)\n",
        "\n",
        "cv2.imshow('Window',read_img)  #display image\n",
        "\n",
        "cv2.imwrite(r'C:\\Users\\DCL\\OneDrive\\Documents\\others\\tutorial\\img_1.png',read_img) #output image directory\n",
        "\n",
        "cv2.waitKey(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "accurate-contents",
      "metadata": {
        "id": "accurate-contents"
      },
      "source": [
        "To learn more about the parameters and how the functions work, visit https://learnopencv.com/read-display-and-write-an-image-using-opencv/\n",
        "\n",
        "## Using Pillow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "intelligent-dayton",
      "metadata": {
        "id": "intelligent-dayton"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "read_img_dir = r'D:\\breast\\Benign_Calc\\BenignCalc\\PNG0000.png'  #input image directory\n",
        "\n",
        "read_img = Image.open(read_img_dir) \n",
        "\n",
        "read_img.save(r'C:\\Users\\DCL\\OneDrive\\Documents\\others\\tutorial\\img_2.png')  #output image directory"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cloudy-importance",
      "metadata": {
        "id": "cloudy-importance"
      },
      "source": [
        "To learn more about the parameters and how the functions work, visit https://pillow.readthedocs.io/en/stable/reference/Image.html"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hazardous-africa",
      "metadata": {
        "id": "hazardous-africa"
      },
      "source": [
        "## Reading and writing all images from directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mexican-groove",
      "metadata": {
        "id": "mexican-groove"
      },
      "outputs": [],
      "source": [
        "folder = r\"input folder\"\n",
        "j=1\n",
        "for im in os.listdir(folder):\n",
        "    path = os.path.join(folder,im)\n",
        "    img = cv2.imread(path)\n",
        "    cv2.imwrite(os.path.join(r'output folder', 'image name '+str(j)+'.jpg'),img)\n",
        "    j = j+1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "driven-advancement",
      "metadata": {
        "id": "driven-advancement"
      },
      "source": [
        "## Reading and writing specific number of images from directory\n",
        "\n",
        "Here showed how to work with 15 images from directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "facial-arizona",
      "metadata": {
        "id": "facial-arizona"
      },
      "outputs": [],
      "source": [
        "folder = r\"input folder\"\n",
        "image number = 15\n",
        "j=1\n",
        "for im in os.listdir(folder):\n",
        "    path = os.path.join(folder,im)\n",
        "    img = cv2.imread(path)\n",
        "    cv2.imwrite(os.path.join(r'output folder', 'image name '+str(j)+'.jpg'),img)\n",
        "    j = j+1\n",
        "    if j == image number\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "interim-attendance",
      "metadata": {
        "id": "interim-attendance"
      },
      "source": [
        "## Image Processing\n",
        "\n",
        "Several algorithms are used for artifacts and noise removal and pixel enhancement.\n",
        "\n",
        "**Artifacts Removal**\n",
        "\n",
        "* Binary Masking\n",
        "* Morphological Opening\n",
        "* Largest Contour Detection\n",
        "* Houghline Transformation\n",
        "\n",
        "**Image Enhancement**\n",
        "\n",
        "* Gamma Correction\n",
        "* CLAHE-1st\n",
        "* CLAHE-2nd\n",
        "* Green Fire Blue (ImageJ Filter)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "spare-occasions",
      "metadata": {
        "id": "spare-occasions"
      },
      "source": [
        "## Binary Masking\n",
        "\n",
        "We find some images with thin white border either on the top, bottom, left or right side. Cropping images using a ratio could remove the border but the images would be resized as well. Hence, binary masking is applied with a rectangular mask.\n",
        "\n",
        "To learn more \n",
        "\n",
        "* https://pyimagesearch.com/2021/01/19/image-masking-with-opencv/\n",
        "* https://docs.opencv.org/4.x/dc/da5/tutorial_py_drawing_functions.html\n",
        "* https://machinelearningknowledge.ai/learn-to-draw-rectangle-in-opencv-python-with-cv2-rectangle-with-examples/\n",
        "\n",
        "**Figure below showes the Thin borders of the mammograms, (a) border at top, (b) border at left, and (c) border at bottom**\n",
        "\n",
        "![biology-10-01347-g004-550.jpg](attachment:biology-10-01347-g004-550.jpg)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "received-brass",
      "metadata": {
        "id": "received-brass"
      },
      "outputs": [],
      "source": [
        "### Import necessary libraries \n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image,ImageEnhance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hundred-lesbian",
      "metadata": {
        "id": "hundred-lesbian",
        "outputId": "718393b3-2bd9-44ab-9079-60c1223ee649"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "### Read and show raw image\n",
        "\n",
        "path = r'C:\\Users\\DCL\\OneDrive\\Documents\\others\\tutorial\\a.png'\n",
        "raw_img= cv2.imread(path,0) #read in gray scale format\n",
        "\n",
        "plt.imshow(cv2.cvtColor(raw_img, cv2.COLOR_BGR2RGB))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "legal-croatia",
      "metadata": {
        "id": "legal-croatia",
        "outputId": "b4a10783-01be-4d63-e504-2c1882daca58"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "### Remove boarder of raw image to isolate the breast part from image boarder\n",
        "\n",
        "#give directory of raw image\n",
        "\n",
        "raw_img= cv2.imread(path,0)\n",
        "mask = np.zeros(raw_img.shape, np.uint8)            \n",
        "mask = cv2.rectangle(mask, (2, 2) , (222, 222), (255,255,255), 5) # creating the binary mask\n",
        "mask = cv2.flip(mask,1)\n",
        "mask = cv2.bitwise_not(mask)\n",
        "boarder_removed = cv2.bitwise_and(raw_img, mask)\n",
        "\n",
        "# save boarder removed image to directory\n",
        "cv2.imwrite(r'C:\\Users\\DCL\\OneDrive\\Documents\\others\\tutorial\\border_removed.jpg',boarder_removed)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rural-burner",
      "metadata": {
        "id": "rural-burner"
      },
      "source": [
        "The cv2.rectangle () method is used to create a rectangular mask of the same dimensions as our input image. This method requires five parameters, input_image, start_point, end_point, border_color and border_thickness. Input_image is the image to which the mask will be applied, start_point refers to the starting coordinates of the rectangular mask (x and y coordinates), end_point represents the end coordinates of the rectangular mask (x and y coordinates), border_color is the color of the border, and border_thickness is the thickness of the border.\n",
        "\n",
        "The thickness of the binary mask = 5 px so that the border is removed without losing any necessary pixels. After creating the mask, a bitwise_AND is performed to apply the mask on original images. Bitwise AND requires two parameters, the original image and the binary mask. For any bit presented in the input, the output bit will be 0 if the mask bit is 0."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "suspended-console",
      "metadata": {
        "id": "suspended-console"
      },
      "source": [
        "## Morphological Opening \n",
        "\n",
        "\n",
        "Morphological Opening is one of the morphological operations which is applied to remove small noise and artifacts from an image and smooth the border of targeted larger object.\n",
        "\n",
        "To learn more\n",
        "\n",
        "* https://docs.opencv.org/3.4/d9/d61/tutorial_py_morphological_ops.html\n",
        "\n",
        "* https://medium.com/swlh/image-processing-with-python-morphological-operations-26b7006c0359\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "unexpected-counter",
      "metadata": {
        "id": "unexpected-counter",
        "outputId": "10456857-8965-46a7-a7b4-7782b7b7db0e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Performing morphological opening operation to minimize artefacts\n",
        "\n",
        "# Give directory to boarder_removed image\n",
        "path1 = r'C:\\Users\\DCL\\OneDrive\\Documents\\others\\tutorial\\border_removed.jpg'\n",
        "boarder_removed = cv2.imread(path1,0)\n",
        "kernel = np.ones((20,20),np.uint8)\n",
        "ret,thresh1 = cv2.threshold(boarder_removed,0,255,cv2.THRESH_BINARY)\n",
        "mask_open = cv2.morphologyEx(thresh1, cv2.MORPH_OPEN, kernel)\n",
        "boarder_open = cv2.bitwise_and(boarder_removed, mask_open)\n",
        "\n",
        "# save boarder_open image to directory\n",
        "cv2.imwrite(r'C:\\Users\\DCL\\OneDrive\\Documents\\others\\tutorial\\opening.jpg',boarder_open)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "searching-trademark",
      "metadata": {
        "id": "searching-trademark"
      },
      "source": [
        "**The step by step process of morphological opening can be seen in the figure below**\n",
        "![stack%20opening.png](attachment:stack%20opening.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "separated-underground",
      "metadata": {
        "id": "separated-underground"
      },
      "source": [
        "                   1                             2                             3                           4"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "remarkable-question",
      "metadata": {
        "id": "remarkable-question"
      },
      "source": [
        "In this figure:\n",
        "\n",
        "* image 1 = original image\n",
        "\n",
        "* image 2 = binary image with a threshold value of 0-255 using cv2.threshold (noise and artifacts are visible)\n",
        "\n",
        "* image 3 = noise and artifact free binary mask using cv2.morphologyEx (after applying morphological opening)\n",
        "\n",
        "* image 4 = output of merging image 3 and image 1 (cv2.bitwise_and)\n",
        "\n",
        "In this process, after reading an image with opencv, it is formated into grayscale format. After that, the image is binarized using cv2.threshold so that  all the pixel values turn into 0 and 1.\n",
        "It is found that for a kernel size of (20, 20), results in the optimal outcome. The algorithm is applied using the cv2.morphologyEx function. Along with kernel size, this function requires another parameter, type of morphological operation. As morphological opening is applied, cv2.MORPH_OPEN is used. Thus, the noises are removed from the binarized image and the noise-free mask is then merged with the original image with the help of bitwise_AND function.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "crude-africa",
      "metadata": {
        "id": "crude-africa"
      },
      "source": [
        "## Largest Contour Detection\n",
        "\n",
        "After the completion of morphological opening, some images still contain large artefacts that the morphological opening could not remove. We therefore use another approach named largest contour detection to extract only the breast portition. Using openCV, contours can be detected marked and the largest contour can be extracted using cv2.findContours, cv2.drawContours and max functions.\n",
        "\n",
        "To learn more\n",
        "\n",
        "* http://www.learningaboutelectronics.com/Articles/How-to-find-the-largest-or-smallest-object-in-an-image-Python-OpenCV.php\n",
        "* https://learnopencv.com/contour-detection-using-opencv-python-c/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "radio-killer",
      "metadata": {
        "id": "radio-killer",
        "outputId": "54f47ca4-9baa-4a3a-da14-b8206b18ac43"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "### Extract Breast portion of image\n",
        "\n",
        "# give directory to boarder_open image\n",
        "path = r'C:\\Users\\DCL\\OneDrive\\Documents\\others\\tutorial\\opening.jpg' # image after applying morphological opening\n",
        "original = cv2.imread(path)\n",
        "boarder_open = Image.open(path) \n",
        "\n",
        "img_brightness_obj=ImageEnhance.Brightness(boarder_open)\n",
        "enhanced_img=img_brightness_obj.enhance(8)\n",
        "\n",
        "img  = cv2.cvtColor(np.array(enhanced_img), cv2.COLOR_BGR2RGB)\n",
        "image_contours = np.zeros((original.shape[1], original.shape[0], 1), np.uint8)\n",
        "image_binary = np.zeros((original.shape[1], original.shape[0], 1), np.uint8)\n",
        "\n",
        "for channel in range(original.shape[2]):\n",
        "        ret, image_thresh = cv2.threshold(img[:, :, channel], 127, 255, cv2.THRESH_BINARY)    \n",
        "        contours = cv2.findContours(image_thresh, 1, 1)[0]   \n",
        "        cv2.drawContours(image_contours, contours, -1, (255,255,255), 3)  \n",
        "        \n",
        "contours = cv2.findContours(image_contours, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)[0]\n",
        "cv2.drawContours(image_binary, [max(contours, key = cv2.contourArea)],-1, (255, 255, 255), -1)\n",
        "rgb_img = cv2.cvtColor(image_binary, cv2.COLOR_GRAY2RGB)\n",
        "\n",
        "largest_contour = cv2.bitwise_and(original,rgb_img)\n",
        "\n",
        "\n",
        "# save largest_contour image to directory\n",
        "cv2.imwrite(r'C:\\Users\\DCL\\OneDrive\\Documents\\others\\tutorial\\largest_contour.jpg',largest_contour)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "several-credit",
      "metadata": {
        "id": "several-credit"
      },
      "source": [
        "**The step by step process of extracting largest contour can be seen in the figure below**\n",
        "\n",
        "![largest.jpg](attachment:largest.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "parliamentary-deadline",
      "metadata": {
        "id": "parliamentary-deadline"
      },
      "source": [
        "In this figure:\n",
        "\n",
        "* image 1 = original image\n",
        "\n",
        "* image 2 = artifact free binary mask using after extracting the largest contour \n",
        "\n",
        "* image 3 = output of merging image 2 and image 1 (cv2.bitwise_and)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "crazy-november",
      "metadata": {
        "id": "crazy-november"
      },
      "source": [
        "## Hough Line Transform\n",
        "\n",
        "![largest_contour.jpg](attachment:largest_contour.jpg)\n",
        "\n",
        "As can be seen from this image a bright thin line is attached to the breast contour. This is an unnecessary region which can be removed using Hough Line Transform. Hough Line Transform can detect any shape,using given parameters, even if it is broken or distorted.\n",
        "\n",
        "To learn more : https://opencv24-python-tutorials.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_houghlines/py_houghlines.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "decent-while",
      "metadata": {
        "id": "decent-while"
      },
      "outputs": [],
      "source": [
        "### remove bright lines\n",
        "\n",
        "img1=cv2.imread(r'C:\\Users\\DCL\\OneDrive\\Documents\\others\\tutorial\\largest_contour.jpg')\n",
        "img = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n",
        "img = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n",
        "    \n",
        "m=np.max(img1)\n",
        "n=255\n",
        "l=int(m-60)\n",
        "    \n",
        "white = (n,n,n)\n",
        "black = (0,0,l)\n",
        "mask = cv2.inRange(img, black,white)\n",
        "\n",
        "cv2.imwrite(r\"C:\\Users\\DCL\\OneDrive\\Documents\\others\\tutorial\\inRange_mask.jpg\", mask)\n",
        "\n",
        "img0 = cv2.imread(r'C:\\Users\\DCL\\OneDrive\\Documents\\others\\tutorial\\largest_contour.jpg',0)\n",
        "img = cv2.imread(r\"C:\\Users\\DCL\\OneDrive\\Documents\\others\\tutorial\\inRange_mask.jpg\",0)\n",
        "edges = cv2.Canny(img,50,150,apertureSize = 3)\n",
        "try:\n",
        "    lines = cv2.HoughLines(edges,1,np.pi/50,50)\n",
        "    for rho,theta in lines[0]:\n",
        "        a = np.cos(theta)\n",
        "        b = np.sin(theta)\n",
        "        x0 = a*rho\n",
        "        y0 = b*rho\n",
        "        x1 = int(x0 + 1000*(-b))\n",
        "        y1 = int(y0 + 1000*(a))\n",
        "        x2 = int(x0 - 1000*(-b))\n",
        "        y2 = int(y0 - 1000*(a))\n",
        "        cv2.line(img0,(x1,y1),(x2,y2),(0,0,255),4) \n",
        "        \n",
        "        cv2.imwrite(r\"C:\\Users\\DCL\\OneDrive\\Documents\\others\\tutorial\\line removed.jpg\",img0)\n",
        "        \n",
        "        edges = cv2.Canny(img0,50,150,apertureSize = 3)\n",
        "        try:\n",
        "            lines = cv2.HoughLines(edges,1,np.pi/50,50)\n",
        "            for rho,theta in lines[0]:\n",
        "                a = np.cos(theta)\n",
        "                b = np.sin(theta)\n",
        "                x0 = a*rho\n",
        "                y0 = b*rho\n",
        "                x1 = int(x0 + 1000*(-b))\n",
        "                y1 = int(y0 + 1000*(a))\n",
        "                x2 = int(x0 - 1000*(-b))\n",
        "                y2 = int(y0 - 1000*(a))\n",
        "                cv2.line(img0,(x1,y1),(x2,y2),(0,0,255),10) \n",
        "\n",
        "            cv2.imwrite(r\"C:\\Users\\DCL\\OneDrive\\Documents\\others\\tutorial\\line removed.jpg\",img0)\n",
        "        \n",
        "        except:\n",
        "            cv2.imwrite(r\"C:\\Users\\DCL\\OneDrive\\Documents\\others\\tutorial\\line removed.jpg\",img0)\n",
        "        \n",
        "except:\n",
        "    cv2.imwrite(r\"C:\\Users\\DCL\\OneDrive\\Documents\\others\\tutorial\\no line.jpg\",img0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sensitive-mother",
      "metadata": {
        "id": "sensitive-mother"
      },
      "source": [
        "**The step by step process of Hough Line Transform can be seen in the figure below**\n",
        "![stack%20contour.jpg](attachment:stack%20contour.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "directed-appeal",
      "metadata": {
        "id": "directed-appeal"
      },
      "source": [
        "In this figure:\n",
        "\n",
        "* image 1 = original image\n",
        "\n",
        "* image 2 = extracted bright line to be removed using Hough Line Transform\n",
        "\n",
        "* image 3 = output applying Hough Line Transform on original image using image 2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "manufactured-server",
      "metadata": {
        "id": "manufactured-server"
      },
      "source": [
        "## Image Enhancement\n",
        "\n",
        "## Gamma Correction\n",
        "\n",
        "Gamma Correction is an image enhancement technique that adjusts the overall brightness and contrast of an image. By altering this gamma value, the optimal brightness and contrast can be acquired in order to detect objects more precisely.\n",
        "\n",
        "In this process, after reading an image with opencv, it is formated into grayscale format. Using the function of np.array gamma correction is applied.\n",
        "\n",
        "To learn about Gamma Correction in details check the paper https://www.mdpi.com/2079-7737/10/12/1347\n",
        "\n",
        "To learn Gamma Correction about the functions and parameters visit https://pyimagesearch.com/2015/10/05/opencv-gamma-correction/#:~:text=There%20are%20two%20(easy)%20ways,range%20%5B0%2C%20255%5D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "coupled-guarantee",
      "metadata": {
        "id": "coupled-guarantee",
        "outputId": "8e8b29a0-fea5-40ac-b669-9bcbded43482"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "read_img_dir = r\"C:\\Users\\DCL\\OneDrive\\Documents\\others\\tutorial\\line removed.jpg\"  #input image directory\n",
        "\n",
        "read_img = cv2.imread(read_img_dir)\n",
        "\n",
        "g = cv2.cvtColor(read_img, cv2.COLOR_BGR2GRAY)  #GRAYSCALE format\n",
        "\n",
        "gamma = np.array(255*(g / 255) ** 2.0, dtype = 'uint8')  # applying gamma correction using value = 2\n",
        "\n",
        "cv2.imwrite(r'C:\\Users\\DCL\\OneDrive\\Documents\\others\\tutorial\\gamma.png',gamma) #output image directory\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "thirty-archive",
      "metadata": {
        "id": "thirty-archive"
      },
      "source": [
        "You can see the amazing transformation below using a gamma value of 2. The left image is before applying gamma correction and the right image is after applying gamma correction. \n",
        "\n",
        "Here,\n",
        "\n",
        "gamma value > 1 means brightening.\n",
        "\n",
        "gamma value = 1 means no effect.\n",
        "\n",
        "gamma value < 1 means darkening.\n",
        "\n",
        "\n",
        "However, a particular gamma may not be suitable for any dataset. The suitable value can be chosen by testing with a few gamma value with the images.\n",
        "\n",
        "**Tranformation after applying gamma correction**\n",
        "\n",
        "![gamma%20stack.png](attachment:gamma%20stack.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "polyphonic-helicopter",
      "metadata": {
        "id": "polyphonic-helicopter"
      },
      "source": [
        "## Contrast limited adaptive histogram equalization (CLAHE)\n",
        "\n",
        "CLAHE is an image enhancement technique which is used to improve the visibility level of image or video.\n",
        "\n",
        "In this process, after reading an image with opencv, it is formated into grayscale format. Using the function of cv2.createCLAHE CLAHE is applied. There are two paramaters in this function, which can be altered and tested with different values to get the optimal output.\n",
        "\n",
        "In this step, we will be applying CLAHE twice as in this process, the Region of interest (ROI) gets visible more.\n",
        "\n",
        "**For more explanation** \n",
        "\n",
        "* https://en.wikipedia.org/wiki/Adaptive_histogram_equalization\n",
        "\n",
        "* https://pyimagesearch.com/2021/02/01/opencv-histogram-equalization-and-adaptive-histogram-equalization-clahe/\n",
        "\n",
        "* https://www.mdpi.com/2079-7737/10/12/1347\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "figured-lunch",
      "metadata": {
        "id": "figured-lunch",
        "outputId": "d6cd1294-8f0c-4717-8c2a-eaca229ef52b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "### Apply CLAHE (2 times) on artefacts_removed gamma images\n",
        "\n",
        "path = r'C:\\Users\\DCL\\OneDrive\\Documents\\others\\tutorial\\gamma.png'\n",
        "img = cv2.imread(path,0)\n",
        "\n",
        "clahe = cv2.createCLAHE(clipLimit=1.0, tileGridSize=(8,8))\n",
        "clahe_1 = clahe.apply(img)\n",
        "cv2.imwrite(r'C:\\Users\\DCL\\OneDrive\\Documents\\others\\tutorial\\clahe_1.png',clahe_1)\n",
        "\n",
        "clahe_2 = clahe.apply(clahe_1)\n",
        "\n",
        "cv2.imwrite(r'C:\\Users\\DCL\\OneDrive\\Documents\\others\\tutorial\\clahe_2.png',clahe_2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "floral-average",
      "metadata": {
        "id": "floral-average"
      },
      "source": [
        "**The step by step process of applying CLAHE can be seen in the figure below**\n",
        "![clahe%20stack.png](attachment:clahe%20stack.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "environmental-credit",
      "metadata": {
        "id": "environmental-credit"
      },
      "source": [
        "In this figure:\n",
        "\n",
        "* image 1 = image after applying gamma correction\n",
        "\n",
        "* image 2 = after appying CLAHE on image 1\n",
        "\n",
        "* image 3 = oafter appying CLAHE on image 2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "going-organization",
      "metadata": {
        "id": "going-organization"
      },
      "source": [
        "## Green Fire Blue (ImageJ Filter)\n",
        "\n",
        "Using ImageJ software You can display, annotate, edit,  measure, analyze, process, print, and save image data. There are several filters which can be applied to highlight ROI from the surronding pixels.\n",
        "\n",
        "The software can be downloaded from here https://imagej.en.softonic.com/download\n",
        "\n",
        "For more Information visit https://imagej.nih.gov/ij/download.html\n",
        "\n",
        "After installing the software, you will a page like this will be appeared.\n",
        "\n",
        "![imj1.png](attachment:imj1.png)\n",
        "\n",
        "Several options can be seen here such as File, Edit, Image, process etc. You may explore the options with an image to see how they work. For now will be sticking with Green Fire Blue (ImageJ Filter). By clicking on the **File** option an image can be loaded from computer directory."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "laughing-lease",
      "metadata": {
        "id": "laughing-lease"
      },
      "source": [
        "![IMJ4.png](attachment:IMJ4.png)\n",
        "\n",
        "Afterwards, Go to option **Image --> Lookup tables**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "assisted-cloud",
      "metadata": {
        "id": "assisted-cloud"
      },
      "source": [
        "![IMJ3.png](attachment:IMJ3.png)\n",
        "\n",
        "On **Lookup Tables** you can see several image filter. By testing the filters with image, the suitable one can be selected. For now, we will apply **Green Fire Blue**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fallen-george",
      "metadata": {
        "id": "fallen-george"
      },
      "source": [
        "![IMG5.png](attachment:IMG5.png)\n",
        "\n",
        "It can be seen how **Green Fire Blue** filter highlighted the ROI in a different pixel value from surronding tissues.\n",
        "\n",
        "The last step is saving the image on directory. you can save image in any format such as .png, .jpeg etc. See the image below."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "continuing-mason",
      "metadata": {
        "id": "continuing-mason"
      },
      "source": [
        "![IMG6.png](attachment:IMG6.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "continuing-plastic",
      "metadata": {
        "id": "continuing-plastic"
      },
      "source": [
        "## Image Augmentation\n",
        "\n",
        "We employ seven augmentation methods on the pre-processed dataset to increase the number of images:\n",
        "\n",
        "* vertical flipping\n",
        "* horizontal flipping\n",
        "* horizontal-vertical flipping\n",
        "* rotating 30°\n",
        "* rotating 30°-horizontal flip\n",
        "* rotating −30°\n",
        "* rotating −30°-horizontal flip\n",
        "\n",
        "\n",
        "Learn more\n",
        "\n",
        "* https://www.thepythoncode.com/article/image-transformations-using-opencv-in-python\n",
        "* https://note.nkmk.me/en/python-opencv-numpy-rotate-flip/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "angry-specialist",
      "metadata": {
        "id": "angry-specialist",
        "outputId": "3d3361f8-b449-438c-eb98-063768405844"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import imutils\n",
        "\n",
        "path = r'C:\\Users\\DCL\\OneDrive\\Documents\\others\\tutorial\\gfb.png'\n",
        "img = cv2.imread(path)\n",
        "\n",
        "    \n",
        "    #0 means flipping around the x-axis \n",
        "    #positive value 1 means flipping around y-axis. \n",
        "    #Negative value (for example, -1) means flipping around both axes.\n",
        "    \n",
        "f1= cv2.flip(img,0) # vertical flipping\n",
        "cv2.imwrite(r'C:\\Users\\DCL\\OneDrive\\Documents\\others\\tutorial\\gfb 1.png',f1)\n",
        "\n",
        "f2= cv2.flip(img,-1) # horizontal flipping\n",
        "cv2.imwrite(r'C:\\Users\\DCL\\OneDrive\\Documents\\others\\tutorial\\gfb 2.png',f2)\n",
        "\n",
        "f3= cv2.flip(img,1) # horizontal-vertical flipping\n",
        "cv2.imwrite(r'C:\\Users\\DCL\\OneDrive\\Documents\\others\\tutorial\\gfb 1.png',f3)\n",
        "\n",
        "f4 = imutils.rotate(img, angle=30) #custom rotation ang - rotating 30°\n",
        "cv2.imwrite(r'C:\\Users\\DCL\\OneDrive\\Documents\\others\\tutorial\\gfb 1.png',f4)\n",
        "\n",
        "f5 = imutils.rotate(f1, angle=30) #custom rotation ang - rotating 30°-horizontal flip\n",
        "cv2.imwrite(r'C:\\Users\\DCL\\OneDrive\\Documents\\others\\tutorial\\gfb 1.png',f5)\n",
        "\n",
        "f6 = imutils.rotate(img, angle= 330) #custom rotation ang - rotating −30°\n",
        "cv2.imwrite(r'C:\\Users\\DCL\\OneDrive\\Documents\\others\\tutorial\\gfb 1.png',f6)\n",
        "f7 = imutils.rotate(f1, angle= 330) #custom rotation ang - rotating −30°-horizontal flip\n",
        "cv2.imwrite(r'C:\\Users\\DCL\\OneDrive\\Documents\\others\\tutorial\\gfb 1.png',f7)\n",
        "      "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "secure-session",
      "metadata": {
        "id": "secure-session"
      },
      "source": [
        "After applying the above transformation, for a single image seven transformed images will be obtained.\n",
        "\n",
        "![biology-10-01347-g017-550.webp](attachment:biology-10-01347-g017-550.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "applicable-movie",
      "metadata": {
        "id": "applicable-movie"
      },
      "source": [
        "## Dataset Split\n",
        "\n",
        "Next step is splitting the dataset into train, validation and test to train the model. The commonly used splitting rations are\n",
        "\n",
        "* train:val:test = 70:20:10, 80:,10:10, 70:10:20 etc.\n",
        "\n",
        "For our study, we will split the dataset using a ration of train:val:test = 70:10:20 using python's splitfolders library. All you have to do is\n",
        "\n",
        "* import library\n",
        "* define the input folder directory containing all the four classes\n",
        "* define an empty output folder directory\n",
        "* define the ratio\n",
        "\n",
        "splitfolder will automatically create three folder named train, val and test on your output directory and save the images there after splitting.\n",
        "\n",
        "To learn more : https://pypi.org/project/split-folders/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "turned-indonesian",
      "metadata": {
        "id": "turned-indonesian"
      },
      "outputs": [],
      "source": [
        "import splitfolders\n",
        "\n",
        "input_folder=\"input dataset directory containing all the four classes\"\n",
        "\n",
        "\n",
        "output=\"output directory\"\n",
        "\n",
        "\n",
        "splitfolders.ratio(input_folder, output, seed=42, ratio=(.70, .10, .20))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "previous-serial",
      "metadata": {
        "id": "previous-serial"
      },
      "source": [
        "## Model Building\n",
        "\n",
        "As our dataset is pre-processed and augmented, the final step is classification of the diseases using deep learning. We will employ a CNN model to classify. A CNN model is mainly consists of Convolutional layer, maxpool layer, Flatten layer and Fully Connected layer. \n",
        "To know more about CNN architecture visit:\n",
        "\n",
        "* https://www.upgrad.com/blog/basic-cnn-architecture/\n",
        "* https://towardsdatascience.com/convolutional-neural-networks-explained-9cc5188c4939\n",
        "* https://www.analyticsvidhya.com/blog/2020/10/what-is-the-convolutional-neural-network-architecture/\n",
        "\n",
        "You can build a CNN architecture using Keras library of Python:\n",
        "\n",
        "* https://towardsdatascience.com/building-a-convolutional-neural-network-cnn-in-keras-329fbbadc5f5\n",
        "* https://www.analyticsvidhya.com/blog/2021/06/building-a-convolutional-neural-network-using-tensorflow-keras/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "worldwide-project",
      "metadata": {
        "id": "worldwide-project"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.layers import Conv2D,MaxPool2D,Dropout,Flatten,Dense,BatchNormalization\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping, CSVLogger"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "worldwide-institution",
      "metadata": {
        "id": "worldwide-institution"
      },
      "source": [
        "If the libraries can not be imported, you have to install first using pip that we have already discussed.\n",
        "\n",
        "After importing the libary, we have to define the input image size and the number of classes. in this case we have defined\n",
        "\n",
        "* input_shape=(224, 224, 3)\n",
        "* class_num = 4\n",
        "\n",
        "First we will construct a base CNN model with randomly applied number of layers and other hyper-parameters. Later, step by step we will apply ablation study altering different components of the base model to improve accuracy. Finally, the optimal model configuration will be generated based on highest performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "australian-stick",
      "metadata": {
        "id": "australian-stick",
        "outputId": "a2f469b3-0645-4d03-f6b6-56dadf173c94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_20 (Conv2D)           (None, 222, 222, 64)      1792      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_15 (MaxPooling (None, 111, 111, 64)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_21 (Conv2D)           (None, 109, 109, 128)     73856     \n",
            "_________________________________________________________________\n",
            "conv2d_22 (Conv2D)           (None, 107, 107, 64)      73792     \n",
            "_________________________________________________________________\n",
            "conv2d_23 (Conv2D)           (None, 105, 105, 32)      18464     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_16 (MaxPooling (None, 52, 52, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_24 (Conv2D)           (None, 50, 50, 32)        9248      \n",
            "_________________________________________________________________\n",
            "conv2d_25 (Conv2D)           (None, 48, 48, 32)        9248      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_17 (MaxPooling (None, 24, 24, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_26 (Conv2D)           (None, 22, 22, 64)        18496     \n",
            "_________________________________________________________________\n",
            "conv2d_27 (Conv2D)           (None, 20, 20, 32)        18464     \n",
            "_________________________________________________________________\n",
            "conv2d_28 (Conv2D)           (None, 18, 18, 16)        4624      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_18 (MaxPooling (None, 9, 9, 16)          0         \n",
            "_________________________________________________________________\n",
            "flatten_5 (Flatten)          (None, 1296)              0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 1024)              1328128   \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 4)                 4100      \n",
            "=================================================================\n",
            "Total params: 1,560,212\n",
            "Trainable params: 1,560,212\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "class_num = 4\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(64,(3,3),activation='relu',input_shape=(224,224,3)))\n",
        "model.add(MaxPool2D(2,2))\n",
        "\n",
        "model.add(Conv2D(128,(3,3),activation='relu'))\n",
        "model.add(Conv2D(64,(3,3),activation='relu'))\n",
        "model.add(Conv2D(32,(3,3),activation='relu'))\n",
        "model.add(MaxPool2D(2,2))\n",
        "\n",
        "model.add(Conv2D(32,(3,3),activation='relu'))\n",
        "model.add(Conv2D(32,(3,3),activation='relu'))\n",
        "model.add(MaxPool2D(2,2))\n",
        "\n",
        "model.add(Conv2D(64,(3,3),activation='relu'))\n",
        "model.add(Conv2D(32,(3,3),activation='relu'))\n",
        "model.add(Conv2D(16,(3,3),activation='relu'))\n",
        "model.add(MaxPool2D(2,2))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1024,activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(class_num,activation='softmax'))\n",
        "\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "perfect-accessory",
      "metadata": {
        "id": "perfect-accessory"
      },
      "source": [
        "In this architecture, we have nine convolution layer, four maxpool layer, one flatten layer, one dropout layer, one dense layer and one fully connected (FC) layer. See the below example ov convolution layer:\n",
        "\n",
        "**model.add(Conv2D(128,(3,3),activation='relu'))**\n",
        "\n",
        "\n",
        "* 128 = number of kernels\n",
        "* (3,3) = size of kernels\n",
        "* activation='relu'\n",
        "\n",
        "**model.add(MaxPool2D(2,2))**\n",
        "\n",
        "* MaxPool2D = type of pooling layer\n",
        "* (2,2) = size of kernels\n",
        "\n",
        "**model.add(Flatten())**\n",
        "\n",
        "* Flatten() = type of flatten layer\n",
        "\n",
        "**model.add(Dense(1024,activation='relu'))**\n",
        "\n",
        "* Dense(1024,activation='relu') = Dense layer with 1024 neorons and relu activation function\n",
        "\n",
        "**model.add(Dropout(0.5))**\n",
        "\n",
        "* Dropout(0.5) = Dropout layer with value of 0.5\n",
        "\n",
        "**model.add(Dense(class_num,activation ='softmax'))**\n",
        "\n",
        "* class_num = number of classes in the dataset\n",
        "* activation ='softmax' = 'softmax' activation function for FC layer (final classification)\n",
        "\n",
        "In ablation study process, we will alter all of these components to improve performance.\n",
        "\n",
        "However, let's compile the model first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hydraulic-amino",
      "metadata": {
        "id": "hydraulic-amino"
      },
      "outputs": [],
      "source": [
        "op = tf.keras.optimizers.Adam(lr=0.001)\n",
        "model.compile(\n",
        "  loss='categorical_crossentropy',\n",
        "  optimizer=op,\n",
        "  metrics=['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "innovative-literacy",
      "metadata": {
        "id": "innovative-literacy"
      },
      "source": [
        "Here,\n",
        "\n",
        "* tf.keras.optimizers.Adam(lr=0.001) = optimizer Adam and learning rate, lr=0.001\n",
        "\n",
        "* loss ='categorical_crossentropy' = loss function is categorical_crossentropy. This is mostly used in multi-class classification problem.\n",
        "\n",
        "However, in ablation study process, we will alter all of these components as well.\n",
        "\n",
        "Now, let's load the training_set, val_set and test_set using ImageDataGenerator. To learn more about ImageDataGenerator visit https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator. \n",
        "\n",
        "You can see inside ImageDataGenerator a few augmentation code is given. These augmentation techniques can be changed to improve accuracy. To learn more visit https://www.analyticsvidhya.com/blog/2020/08/image-augmentation-on-the-fly-using-keras-imagedatagenerator/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "confidential-modem",
      "metadata": {
        "id": "confidential-modem"
      },
      "outputs": [],
      "source": [
        "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   shear_range = 0.2,\n",
        "                                   zoom_range = 0.2,\n",
        "                                   vertical_flip= True,\n",
        "                                   horizontal_flip = True)\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "training_set = train_datagen.flow_from_directory('training directory',               #training dataset\n",
        "                                                 target_size = (224,224),\n",
        "                                                 batch_size = 64,\n",
        "                                                 class_mode = 'categorical',\n",
        "                                                 seed=1,\n",
        "                    \n",
        "                                                )\n",
        "\n",
        "val_set = val_datagen.flow_from_directory('validation directory',           #validation dataset\n",
        "                                            target_size = (224,224),\n",
        "                                            batch_size = 64,\n",
        "                                            class_mode = 'categorical')\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "test_set = test_datagen.flow_from_directory('test directory',           #test dataset\n",
        "                                            target_size = (224,224),\n",
        "                                            batch_size = 64,\n",
        "                                            class_mode = 'categorical')\n",
        "\n",
        "\n",
        "model.optimizer.get_config()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "affiliated-alarm",
      "metadata": {
        "id": "affiliated-alarm"
      },
      "source": [
        "Here,\n",
        "\n",
        "* batch_size = 64\n",
        "* class_mode = 'categorical'\n",
        "\n",
        "which also can be changed while conducting ablation study.\n",
        "\n",
        "\n",
        "**We are done with everything now. Let's train the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "prospective-typing",
      "metadata": {
        "id": "prospective-typing"
      },
      "outputs": [],
      "source": [
        "filepath = 'weight.h5'\n",
        "\n",
        "\n",
        "checkpoint1 = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1,save_weights_only=True,\n",
        "                             save_best_only=True, mode='max')\n",
        "\n",
        "\n",
        "\n",
        "log_csv = CSVLogger('weight.csv', separator=',', append=False)\n",
        "    \n",
        "callbacks_list = [checkpoint1,log_csv]\n",
        "\n",
        "\n",
        "r = model.fit_generator(\n",
        "    training_set,\n",
        "    epochs=300,\n",
        "    validation_data=val_set,\n",
        "    steps_per_epoch = len(training_set),\n",
        "    validation_steps=len(val_set),\n",
        "    callbacks=callbacks_list,\n",
        "    shuffle=False\n",
        "    \n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "broke-mounting",
      "metadata": {
        "id": "broke-mounting"
      },
      "source": [
        "While training, Keras saves the model in .h5 format  store the weights and model configuration in a single file. To learn more https://www.tensorflow.org/guide/keras/save_and_serialize. \n",
        "\n",
        "ModelCheckpoint is used to save the configuration for which highest vakidation accuracy is achieved. To learn more https://keras.io/api/callbacks/model_checkpoint/.\n",
        "\n",
        "CSVLogger is used to save the train and validation accuracy, time per epoch in a csv file. To learn more https://keras.io/api/callbacks/csv_logger/\n",
        "\n",
        "The epoch number can be changed based on model's performance.\n",
        "\n",
        "After starting training the model, you will see something like this:\n",
        "\n",
        "![train.png](attachment:train.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "accredited-short",
      "metadata": {
        "id": "accredited-short"
      },
      "source": [
        "Later loss and accuracy curves are derived to detect any overfitting issue occuring while training. These plots also tells how accuracy is improved over the epochs.\n",
        "\n",
        "To learn more https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "injured-leisure",
      "metadata": {
        "id": "injured-leisure"
      },
      "outputs": [],
      "source": [
        "### Plot accuracy and loss curve\n",
        "\n",
        "import matplotlib.image as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# plot the loss\n",
        "plt.plot(r.history['loss'], label='train loss')\n",
        "plt.plot(r.history['val_loss'], label='val loss')\n",
        "plt.title('Training and validation Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.savefig('LossVal_loss')\n",
        "\n",
        "# plot the accuracy\n",
        "plt.plot(r.history['accuracy'], label='train acc')\n",
        "plt.plot(r.history['val_accuracy'], label='val acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.savefig('AccVal_acc')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "compatible-ethnic",
      "metadata": {
        "id": "compatible-ethnic"
      },
      "source": [
        "Final step is deriving the test accuracy. To know about test accuracy visit https://blog.paperspace.com/deep-learning-metrics-precision-recall-accuracy/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "speaking-channel",
      "metadata": {
        "id": "speaking-channel"
      },
      "outputs": [],
      "source": [
        "model.load_weights(\"weight.h5\")\n",
        "preds = model.evaluate_generator(test_set)\n",
        "print (\"Loss = \" + str(preds[0]))\n",
        "print (\"Test Accuracy = \" + str(preds[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "broadband-prediction",
      "metadata": {
        "id": "broadband-prediction"
      },
      "source": [
        "## Ablation study\n",
        "\n",
        "The aim of the ablation study is to acquire a clear understanding of the model’s performance by analyzing the consequence of altering some components. With the alteration of different components or hyper parameters of a model, a change performance is observed. This method can ascertain any potential decrease in the performance of the model which can later be fixed by updating and tuning the network.\n",
        "\n",
        "We have trained our base CNN model several times by altering layer numbers, filer sizes filer numbers, hyper-parameters and parameter values to obtain an optimal performance."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sunrise-sheep",
      "metadata": {
        "id": "sunrise-sheep"
      },
      "source": [
        "**1. changing convolution and maxpool layer**\n",
        "\n",
        "from the base model, the number of convolution and maxpool layer can be decreased and increased to observe the performance. For our case, for the configuration of four convolution and four maxpool layer the highest performance is achieved. The architecture is given below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "brutal-alias",
      "metadata": {
        "id": "brutal-alias"
      },
      "outputs": [],
      "source": [
        "class_num = 4\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(64,(3,3),activation='relu',input_shape=(224,224,3)))\n",
        "model.add(MaxPool2D(2,2))\n",
        "\n",
        "model.add(Conv2D(128,(3,3),activation='relu'))\n",
        "model.add(MaxPool2D(2,2))\n",
        "\n",
        "model.add(Conv2D(32,(3,3),activation='relu'))\n",
        "model.add(MaxPool2D(2,2))\n",
        "\n",
        "model.add(Conv2D(64,(3,3),activation='relu'))\n",
        "model.add(MaxPool2D(2,2))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1024,activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(class_num,activation='softmax'))\n",
        "\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "intimate-cause",
      "metadata": {
        "id": "intimate-cause"
      },
      "source": [
        "**2. changing number of kernels of convolution layers**\n",
        "\n",
        "from the base model, the number of convolution kernel can be decreased and increased to observe the performance. For our case, for the configuration of kernel number **16-->32-->32-->64** the highest performance is achieved. The architecture is given below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "blond-chile",
      "metadata": {
        "id": "blond-chile"
      },
      "outputs": [],
      "source": [
        "class_num = 4\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(16,(3,3),activation='relu',input_shape=(224,224,3)))\n",
        "model.add(MaxPool2D(2,2))\n",
        "\n",
        "model.add(Conv2D(32,(3,3),activation='relu'))\n",
        "model.add(MaxPool2D(2,2))\n",
        "\n",
        "model.add(Conv2D(32,(3,3),activation='relu'))\n",
        "model.add(MaxPool2D(2,2))\n",
        "\n",
        "model.add(Conv2D(64,(3,3),activation='relu'))\n",
        "model.add(MaxPool2D(2,2))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1024,activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(class_num,activation='softmax'))\n",
        "\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "decimal-independence",
      "metadata": {
        "id": "decimal-independence"
      },
      "source": [
        "**3. changing kernel size of convolution layers**\n",
        "\n",
        "from the base model, the convolution kernel size can be decreased and increased to observe the performance. For our case, for kernel size (2,2) the highest performance is achieved. The architecture is given below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "broken-frequency",
      "metadata": {
        "id": "broken-frequency"
      },
      "outputs": [],
      "source": [
        "class_num = 4\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(16,(2,2),activation='relu',input_shape=(224,224,3)))\n",
        "model.add(MaxPool2D(2,2))\n",
        "\n",
        "model.add(Conv2D(32,(2,2),activation='relu'))\n",
        "model.add(MaxPool2D(2,2))\n",
        "\n",
        "model.add(Conv2D(32,(2,2),activation='relu'))\n",
        "model.add(MaxPool2D(2,2))\n",
        "\n",
        "model.add(Conv2D(64,(2,2),activation='relu''))\n",
        "model.add(MaxPool2D(2,2))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1024,activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(class_num,activation='softmax'))\n",
        "\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "atlantic-pulse",
      "metadata": {
        "id": "atlantic-pulse"
      },
      "source": [
        "**4. changing activation function of convolution layers**\n",
        "\n",
        "Five activation functions,PReLU, ReLU, Leaky ReLU, Tanh and Exponential Linear Units (ELU) are experimented where PRelu performs best with our dataset. The architecture is given below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "forbidden-tablet",
      "metadata": {
        "id": "forbidden-tablet"
      },
      "outputs": [],
      "source": [
        "class_num = 4\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(16,(2,2),activation='PReLU',input_shape=(224,224,3)))\n",
        "model.add(MaxPool2D(2,2))\n",
        "\n",
        "model.add(Conv2D(32,(2,2),activation='PReLU'))\n",
        "model.add(MaxPool2D(2,2))\n",
        "\n",
        "model.add(Conv2D(32,(2,2),activation='PReLU'))\n",
        "model.add(MaxPool2D(2,2))\n",
        "\n",
        "model.add(Conv2D(64,(2,2),activation='PReLU'))\n",
        "model.add(MaxPool2D(2,2))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1024,activation='PReLU'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(class_num,activation='softmax'))\n",
        "\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "extra-morocco",
      "metadata": {
        "id": "extra-morocco"
      },
      "source": [
        "**5. changing the type of pooling layer**\n",
        "\n",
        "Two pooling layers, maxpool and average pool, are evaluated where both pooling layers gained almost same accuracy. Hence, we keep maxpool layer to move forward. The architecture is given below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adjustable-hawaiian",
      "metadata": {
        "id": "adjustable-hawaiian"
      },
      "outputs": [],
      "source": [
        "class_num = 4\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(16,(2,2),activation='PReLU',input_shape=(224,224,3)))\n",
        "model.add(MaxPool2D(2,2))\n",
        "\n",
        "model.add(Conv2D(32,(2,2),activation='PReLU'))\n",
        "model.add(MaxPool2D(2,2))\n",
        "\n",
        "model.add(Conv2D(32,(2,2),activation='PReLU'))\n",
        "model.add(MaxPool2D(2,2))\n",
        "\n",
        "model.add(Conv2D(64,(2,2),activation='PReLU'))\n",
        "model.add(MaxPool2D(2,2))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1024,activation='PReLU'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(class_num,activation='softmax'))\n",
        "\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "young-optimum",
      "metadata": {
        "id": "young-optimum"
      },
      "source": [
        "**6. changing flatten layer**\n",
        "\n",
        "We experimented with Global Max pooling and Global Average pooling flatten layer and found that the previously used flatten layer yielded the highest test accuracy.\n",
        "\n",
        "The architecture is given below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "expensive-shepherd",
      "metadata": {
        "id": "expensive-shepherd"
      },
      "outputs": [],
      "source": [
        "class_num = 4\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(16,(2,2),activation='PReLU',input_shape=(224,224,3)))\n",
        "model.add(MaxPool2D(2,2))\n",
        "\n",
        "model.add(Conv2D(32,(2,2),activation='PReLU'))\n",
        "model.add(MaxPool2D(2,2))\n",
        "\n",
        "model.add(Conv2D(32,(2,2),activation='PReLU'))\n",
        "model.add(MaxPool2D(2,2))\n",
        "\n",
        "model.add(Conv2D(64,(2,2),activation='PReLU'))\n",
        "model.add(MaxPool2D(2,2))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1024,activation='PReLU'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(class_num,activation='softmax'))\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "piano-sheep",
      "metadata": {
        "id": "piano-sheep"
      },
      "source": [
        "**7. Changing Loss Functions**\n",
        "\n",
        "Experimentation with different loss functions, namely Binary Crossentropy, Categorical Crossentropy, Mean Squared Error, Mean Absolute Error, Mean Squared Logarithmic Error and Kullback Leibler Divergence was carried out to select the appropriate loss function for our network. While equipped with Categorical Crossentropy the model gained highest test accuracy. Hence this is chosen. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "advisory-maryland",
      "metadata": {
        "id": "advisory-maryland"
      },
      "outputs": [],
      "source": [
        "op = tf.keras.optimizers.Adam(lr=0.001)\n",
        "model.compile(\n",
        "  loss='categorical_crossentropy',\n",
        "  optimizer=op,\n",
        "  metrics=['accuracy']\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "electoral-korea",
      "metadata": {
        "id": "electoral-korea"
      },
      "source": [
        "**8. Changing Optimizer**\n",
        "\n",
        "Experimentation with different optimizers namely Adam, Nadam, SGD, Adamax and RMSprop is carried out to identify the optimal optimizer and found that Nadam yields the highest accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "accessible-philadelphia",
      "metadata": {
        "id": "accessible-philadelphia"
      },
      "outputs": [],
      "source": [
        "op = tf.keras.optimizers.Nadam(lr=0.001)\n",
        "model.compile(\n",
        "  loss='categorical_crossentropy',\n",
        "  optimizer=op,\n",
        "  metrics=['accuracy']\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "electoral-result",
      "metadata": {
        "id": "electoral-result"
      },
      "source": [
        "**9. Changing Learning Rate**\n",
        "\n",
        "Experimentation with different Learning Rates, 0.001, 0.0001, 0.0008, 0.006 is carried out to identify the optimal optimizer and found that 0.0008 yields the highest accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "equivalent-stockholm",
      "metadata": {
        "id": "equivalent-stockholm"
      },
      "outputs": [],
      "source": [
        "op = tf.keras.optimizers.Nadam(lr=0.0008)\n",
        "model.compile(\n",
        "  loss='categorical_crossentropy',\n",
        "  optimizer=op,\n",
        "  metrics=['accuracy']\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rotary-adrian",
      "metadata": {
        "id": "rotary-adrian"
      },
      "source": [
        "**9. Changing batch size**\n",
        "\n",
        "Experimentation with different batch size, 16,32,64 is carried out to identify the optimal optimizer and found that 32 yields the highest accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "experienced-visitor",
      "metadata": {
        "id": "experienced-visitor"
      },
      "outputs": [],
      "source": [
        "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   shear_range = 0.2,\n",
        "                                   zoom_range = 0.2,\n",
        "                                   vertical_flip= True,\n",
        "                                   horizontal_flip = True)\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "training_set = train_datagen.flow_from_directory('training directory',               #training dataset\n",
        "                                                 target_size = (224,224),\n",
        "                                                 batch_size = 32,\n",
        "                                                 class_mode = 'categorical',\n",
        "                                                 seed=1,\n",
        "                    \n",
        "                                                )\n",
        "\n",
        "val_set = val_datagen.flow_from_directory('validation directory',           #validation dataset\n",
        "                                            target_size = (224,224),\n",
        "                                            batch_size = 32,\n",
        "                                            class_mode = 'categorical')\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "test_set = test_datagen.flow_from_directory('test directory',           #test dataset\n",
        "                                            target_size = (224,224),\n",
        "                                            batch_size = 1,\n",
        "                                            class_mode = 'categorical')\n",
        "\n",
        "\n",
        "model.optimizer.get_config()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "compliant-partnership",
      "metadata": {
        "id": "compliant-partnership"
      },
      "source": [
        "## Optimal Model Architecture\n",
        "\n",
        "Finally, This is our optimal model configuration with highest accuracy after applying ablation study of nine cases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "historical-cathedral",
      "metadata": {
        "id": "historical-cathedral"
      },
      "outputs": [],
      "source": [
        "class_num = 4\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(16,(2,2),activation='PReLU',input_shape=(224,224,3)))\n",
        "model.add(MaxPool2D(2,2))\n",
        "\n",
        "model.add(Conv2D(32,(2,2),activation='PReLU'))\n",
        "model.add(MaxPool2D(2,2))\n",
        "\n",
        "model.add(Conv2D(32,(2,2),activation='PReLU'))\n",
        "model.add(MaxPool2D(2,2))\n",
        "\n",
        "model.add(Conv2D(64,(2,2),activation='PReLU'))\n",
        "model.add(MaxPool2D(2,2))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1024,activation='PReLU'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(class_num,activation='softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "op = tf.keras.optimizers.Nadam(lr=0.0008)\n",
        "model.compile(\n",
        "  loss='categorical_crossentropy',\n",
        "  optimizer=op,\n",
        "  metrics=['accuracy']\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   shear_range = 0.2,\n",
        "                                   zoom_range = 0.2,\n",
        "                                   vertical_flip= True,\n",
        "                                   horizontal_flip = True)\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "training_set = train_datagen.flow_from_directory('training directory',               #training dataset\n",
        "                                                 target_size = (224,224),\n",
        "                                                 batch_size = 32,\n",
        "                                                 class_mode = 'categorical',\n",
        "                                                 seed=1,\n",
        "                    \n",
        "                                                )\n",
        "\n",
        "val_set = val_datagen.flow_from_directory('validation directory',           #validation dataset\n",
        "                                            target_size = (224,224),\n",
        "                                            batch_size = 32,\n",
        "                                            class_mode = 'categorical')\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "test_set = test_datagen.flow_from_directory('test directory',           #test dataset\n",
        "                                            target_size = (224,224),\n",
        "                                            batch_size = 1,\n",
        "                                            class_mode = 'categorical')\n",
        "\n",
        "\n",
        "model.optimizer.get_config()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "southeast-river",
      "metadata": {
        "id": "southeast-river"
      },
      "source": [
        "# Happy Learning!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.4"
    },
    "colab": {
      "name": "Mammogram Classification.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}